# ml-sharp-animator

> **This is a fork of [kstonekuan's ml-sharp-web-viewer](https://github.com/kstonekuan/ml-sharp-web-viewer) repository.**
>
> This version allows you to upload images and videos to be visualized and animated.

## Features

- **Video to 4D**: Upload a video, and the backend will extract frames, convert them to 3D Gaussian Splats, and play them back as a 4D free-viewpoint video.
- **Image to 3D**: Upload a single PNG to convert it to a static 3D Splat.
- **Download**: Download the generated `.ply` files as a ZIP archive.

## Prerequisites

Before running this project, ensure you have the following installed:

1.  **Anaconda or Miniconda**: Required for managing the Python environment. [Install Miniconda](https://docs.conda.io/en/latest/miniconda.html).
2.  **FFmpeg**: Required for video processing.
    - **macOS**: `brew install ffmpeg`
    - **Linux**: `sudo apt install ffmpeg`
    - **Windows**: [Download FFmpeg](https://ffmpeg.org/download.html) and add it to your PATH.
3.  **Node.js & npm**: Required for building the frontend. [Install Node.js](https://nodejs.org/).

## Installation

1.  **Clone the repository**:

    ```bash
    git clone https://github.com/yumobennyyang/ml-sharp-animator.git
    cd ml-sharp-animator
    ```

2.  **Set up the Python Environment**:

    ```bash
    # Create the conda environment
    conda create -n sharp python=3.13

    # Activate the environment
    conda activate sharp

    # Install dependencies
    pip install -r requirements.txt
    ```

3.  **Build the Frontend**:
    ```bash
    cd web
    npm install
    npm run build
    cd ..
    ```

## Running the App

### Using the Helper Script (macOS/Linux)

This is the helper script to easily start the server:

```bash
./run_server.sh
```

### Manual Start

If you prefer to run it manually:

1.  **Activate the environment**:

    ```bash
    conda activate sharp
    ```

2.  **Start the server**:

    ```bash
    uvicorn server.main:app --reload
    ```

3.  **Open your browser**:
    Navigate to [http://localhost:8000](http://localhost:8000)

## Usage

1.  **Upload a Video**: Drag and drop a video file (`.mp4`, `.mov`, `.avi`) to process and display it as a 3D animation.
2.  **Upload an Image**: Drag and drop a single `.png` file to convert and display it as a static 3D Splat.
3.  **Upload a PLY**: Drag and drop a `.ply` file to view an existing 3D Splat.
4.  **Upload Multiple Images**: Drag and drop up to 500 `.png` files to convert and play them as a looped animation at 12 FPS.
5.  **Upload Multiple PLYs**: Drag and drop up to 500 `.ply` files to play them as a looped animation at 12 FPS.
6.  **Download Assets**: Click the "Download PLY(s)" button after processing to save your generated 3D assets as a ZIP file. If the button does not respond, you can manually retrieve the files from the `server/temp_data` directory.

---

## Original README from [kstonekuan/ml-sharp-web-viewer](https://github.com/kstonekuan/ml-sharp-web-viewer)

# ml-sharp-web-viewer

> **This is a fork of [Apple's ml-sharp](https://github.com/apple/ml-sharp) repository.**
>
> This version includes a **web-based Gaussian Splat viewer** that allows you to view `.ply` files generated by ml-sharp directly in your browser.
>
> **Live Demo:** [https://kstonekuan.github.io/ml-sharp-web-viewer/](https://kstonekuan.github.io/ml-sharp-web-viewer/)

## Cloud GPU Inference (No Local GPU Required)

Generate Gaussian splats using [Modal](https://modal.com)'s cloud GPUs. Modal offers a free tier with $30/month in credits.

### Setup (One-Time)

```bash
# Install with cloud support
uv pip install -e ".[cloud]"

# Create Modal account and authenticate
uv run modal token new
```

### Optional: Pre-warm Model Cache

```bash
uv run sharp cloud setup
```

This downloads the ~800MB model to Modal's cloud storage, making subsequent runs faster.

### Usage

```bash
# Run inference on cloud GPU (default: A10 @ $1.10/hr)
uv run sharp cloud predict -i photo.jpg -o output/

# Process multiple images
uv run sharp cloud predict -i photos/ -o output/

# Choose GPU tier
uv run sharp cloud predict -i photo.jpg -o output/ --gpu t4    # $0.59/hr (budget)
uv run sharp cloud predict -i photo.jpg -o output/ --gpu h100  # $3.95/hr (fastest)
```

### Available GPU Tiers

| GPU  | Price/hr | Notes             |
| ---- | -------- | ----------------- |
| T4   | $0.59    | Budget option     |
| L4   | $0.80    | Good value        |
| A10  | $1.10    | Default, balanced |
| A100 | $2.50    | High performance  |
| H100 | $3.95    | Fastest           |

---

## Web Viewer Features

- Upload and view generated `.ply` Gaussian Splat files from ml-sharp
- Multiple camera trajectory animations (rotate, swipe, shake, forward)
- Interactive orbit controls (drag to orbit, scroll to zoom, right-drag to pan)
- Adjustable animation parameters
- No installation required - runs entirely in the browser

---

## Original README from [apple/ml-sharp](https://github.com/apple/ml-sharp)

# Sharp Monocular View Synthesis in Less Than a Second

[![Project Page](https://img.shields.io/badge/Project-Page-green)](https://apple.github.io/ml-sharp/)
[![arXiv](https://img.shields.io/badge/arXiv-2512.10685-b31b1b.svg)](https://arxiv.org/abs/2512.10685)

This software project accompanies the research paper: _Sharp Monocular View Synthesis in Less Than a Second_
by _Lars Mescheder, Wei Dong, Shiwei Li, Xuyang Bai, Marcel Santos, Peiyun Hu, Bruno Lecouat, Mingmin Zhen, Amaël Delaunoy,
Tian Fang, Yanghai Tsin, Stephan Richter and Vladlen Koltun_.

![](data/teaser.jpg)

We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25–34% and DISTS by 21–43% versus the best prior model, while lowering the synthesis time by three orders of magnitude.

## Getting started

We recommend to first create a python environment:

```
conda create -n sharp python=3.13
```

Afterwards, you can install the project using

```
pip install -r requirements.txt
```

To test the installation, run

```
sharp --help
```

## Using the CLI

To run prediction:

```
sharp predict -i /path/to/input/images -o /path/to/output/gaussians
```

The model checkpoint will be downloaded automatically on first run and cached locally at `~/.cache/torch/hub/checkpoints/`.

Alternatively, you can download the model directly:

```
wget https://ml-site.cdn-apple.com/models/sharp/sharp_2572gikvuh.pt
```

To use a manually downloaded checkpoint, specify it with the `-c` flag:

```
sharp predict -i /path/to/input/images -o /path/to/output/gaussians -c sharp_2572gikvuh.pt
```

The results will be 3D gaussian splats (3DGS) in the output folder. The 3DGS `.ply` files are compatible to various public 3DGS renderers. We follow the OpenCV coordinate convention (x right, y down, z forward). The 3DGS scene center is roughly at (0, 0, +z). When dealing with 3rdparty renderers, please scale and rotate to re-center the scene accordingly.

### Rendering trajectories (CUDA GPU only)

Additionally you can render videos with a camera trajectory. While the gaussians prediction works for all CPU, CUDA, and MPS, rendering videos via the `--render` option currently requires a CUDA GPU. The gsplat renderer takes a while to initialize at the first launch.

```
sharp predict -i /path/to/input/images -o /path/to/output/gaussians --render

# Or from the intermediate gaussians:
sharp render -i /path/to/output/gaussians -o /path/to/output/renderings
```

## Evaluation

Please refer to the paper for both quantitative and qualitative evaluations.
Additionally, please check out this [qualitative examples page](https://apple.github.io/ml-sharp/) containing several video comparisons against related work.

## Citation

If you find our work useful, please cite the following paper:

```bibtex
@inproceedings{Sharp2025:arxiv,
  title      = {Sharp Monocular View Synthesis in Less Than a Second},
  author     = {Lars Mescheder and Wei Dong and Shiwei Li and Xuyang Bai and Marcel Santos and Peiyun Hu and Bruno Lecouat and Mingmin Zhen and Ama\"{e}l Delaunoy and Tian Fang and Yanghai Tsin and Stephan R. Richter and Vladlen Koltun},
  journal    = {arXiv preprint arXiv:2512.10685},
  year       = {2025},
  url        = {https://arxiv.org/abs/2512.10685},
}
```

## Acknowledgements

Our codebase is built using multiple opensource contributions, please see [ACKNOWLEDGEMENTS](ACKNOWLEDGEMENTS) for more details.

## License

Please check out the repository [LICENSE](LICENSE) before using the provided code and
[LICENSE_MODEL](LICENSE_MODEL) for the released models.
